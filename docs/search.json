[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "This browser does not support embedded PDFs. You can download the file here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiadan Zhao",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nJiadan Zhao\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJiadan Zhao\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project2/hw1_questions.html",
    "href": "projects/project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nDean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to understand whether matching grants increase charitable giving. Specifically, they collaborated with a liberal nonprofit organization in the United States and sent direct mail solicitations to 50,083 previous donors.\nThese individuals were randomly assigned into:\n\nA control group (received a standard fundraising letter),\nA treatment group (received a letter mentioning a matching grant).\n\nWithin the treatment group, subjects were further randomized along three dimensions:\n\nMatch ratio: 1:1, 2:1, or 3:1 (i.e., for each dollar donated, a matching donor would contribute 1, 2, or 3 dollars).\nMaximum match threshold: $25,000, $50,000, $100,000, or unstated.\nSuggested donation amount: set to their previous highest contribution, or 1.25x / 1.5x of that amount.\n\nThe goal was to determine whether: - The presence of a match offer increases response rate or donation amount, - Higher match ratios further increase donations, - Other factors such as match thresholds or suggested donation amounts influence behavior.\nThe findings were published in the American Economic Review in 2007 and remain widely cited in the economics of charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#introduction",
    "href": "projects/project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nDean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to understand whether matching grants increase charitable giving. Specifically, they collaborated with a liberal nonprofit organization in the United States and sent direct mail solicitations to 50,083 previous donors.\nThese individuals were randomly assigned into:\n\nA control group (received a standard fundraising letter),\nA treatment group (received a letter mentioning a matching grant).\n\nWithin the treatment group, subjects were further randomized along three dimensions:\n\nMatch ratio: 1:1, 2:1, or 3:1 (i.e., for each dollar donated, a matching donor would contribute 1, 2, or 3 dollars).\nMaximum match threshold: $25,000, $50,000, $100,000, or unstated.\nSuggested donation amount: set to their previous highest contribution, or 1.25x / 1.5x of that amount.\n\nThe goal was to determine whether: - The presence of a match offer increases response rate or donation amount, - Higher match ratios further increase donations, - Other factors such as match thresholds or suggested donation amounts influence behavior.\nThe findings were published in the American Economic Review in 2007 and remain widely cited in the economics of charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#data",
    "href": "projects/project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset includes 50,083 prior donors to a liberal nonprofit organization. Each row corresponds to a recipient of a fundraising letter and includes information on their treatment assignment, past donation behavior, and demographics.\nKey summary statistics from the data are shown below. Notable variables include: - treatment: 1 if the individual received a letter mentioning a matching grant - gave: 1 if the individual donated in response to the letter - amount: total amount donated - mrm2: number of months since last donation\nWe use this data to replicate and analyze the effects reported by Karlan & List (2007).\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nfrom scipy.stats import ttest_ind\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Split by treatment group\ncontrol = df[df[\"treatment\"] == 0]\ntreat = df[df[\"treatment\"] == 1]\n\n# t-tests\nt_mrm2 = ttest_ind(control[\"mrm2\"], treat[\"mrm2\"])\nt_freq = ttest_ind(control[\"freq\"], treat[\"freq\"])\n\nt_mrm2, t_freq\n\nimport statsmodels.api as sm\n\n# mrm2 regression\nX_mrm2 = sm.add_constant(df[\"treatment\"])\ny_mrm2 = df[\"mrm2\"]\nmodel_mrm2 = sm.OLS(y_mrm2, X_mrm2).fit()\n\n# freq regression\nX_freq = sm.add_constant(df[\"treatment\"])\ny_freq = df[\"freq\"]\nmodel_freq = sm.OLS(y_freq, X_freq).fit()\n\nmodel_mrm2.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|t|\"]]\nmodel_freq.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|t|\"]]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nP&gt;|t|\n\n\n\n\nconst\n8.047342\n0.088208\n0.000000\n\n\ntreatment\n-0.011979\n0.108021\n0.911702\n\n\n\n\n\n\n\nTo evaluate the success of the randomization, we tested whether key background variables differed between the treatment and control groups. Specifically, we examined:\n\nmrm2: the number of months since the last donation\nfreq: the number of prior donations made by the donor\n\nWe used both t-tests and linear regression."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#experimental-results",
    "href": "projects/project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nFor mrm2, the t-test and regression output returned NaN values, likely due to missing or corrupted entries in the column. We will revisit this variable after inspecting the data.\nFor freq, the regression showed a coefficient on treatment of –0.012, with a p-value of 0.912, indicating no statistically significant difference between groups.\n\nThese results imply that the treatment and control groups were well-balanced with respect to prior donation behavior. This supports the assumption that any difference in outcomes (such as donation rates) can be attributed to the treatment rather than pre-existing differences.\n\nWhy is Table 1 in the paper?\nTable 1 in Karlan & List (2007) serves exactly this purpose: it demonstrates that randomization worked. If treatment and control groups are balanced on observable pre-treatment characteristics, readers can have greater confidence in the causal interpretation of subsequent comparisons. Including a balance table is a standard best practice in experimental papers, especially when the goal is to isolate treatment effects.\n\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Split groups\ncontrol_gave = df[df[\"treatment\"] == 0][\"gave\"]\ntreat_gave = df[df[\"treatment\"] == 1][\"gave\"]\n\n# Run t-test\nttest_ind(control_gave, treat_gave)\n\nTtestResult(statistic=-3.101361000543946, pvalue=0.0019274025949016988, df=50081.0)\n\n\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\nimport statsmodels.api as sm\n\n# Define X and y\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run OLS regression\nmodel = sm.OLS(y, X).fit()\nmodel.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|t|\"]]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nP&gt;|t|\n\n\n\n\nconst\n0.017858\n0.001101\n4.779032e-59\n\n\ntreatment\n0.004180\n0.001348\n1.927403e-03\n\n\n\n\n\n\n\nWe first tested whether the treatment group was more likely to donate using a t-test. The result was statistically significant (p ≈ 0.002), indicating that the difference in donation rates between treatment and control is unlikely to be due to chance.\nTo confirm this, we ran a simple linear regression with gave as the dependent variable and treatment as the explanatory variable. The estimated treatment effect was approximately 0.42 percentage points, meaning that individuals who received a matching grant letter were 0.42 percent more likely to donate. The result was again statistically significant with p less than 0.01.\nThese findings align with Table 2a, Panel A in Karlan and List (2007), and suggest that simply mentioning a matching grant in a fundraising letter can causally increase the probability of donation. This highlights that even subtle framing changes in solicitation messages can meaningfully affect charitable behavior, likely by making the act of giving feel more impactful or urgent.\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\nimport statsmodels.api as sm\n\n#| label: gave-probit\n#| message: false\n#| warning: false\n#| \n# Define X and y\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n# Run Probit model\nprobit_model = sm.Probit(y, X).fit()\nprobit_model.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|z|\"]]\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nP&gt;|z|\n\n\n\n\nconst\n-2.100141\n0.023316\n0.000000\n\n\ntreatment\n0.086785\n0.027879\n0.001852\n\n\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n#| label: ttest-match-ratios\n#| message: false\n#| warning: false\n\nfrom scipy.stats import ttest_ind\n\n# Filter only treated individuals\ntreated = df[df[\"treatment\"] == 1]\n\n# Define match groups\ngave_1to1 = treated[(treated[\"ratio2\"] == 0) & (treated[\"ratio3\"] == 0)][\"gave\"]\ngave_2to1 = treated[treated[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = treated[treated[\"ratio3\"] == 1][\"gave\"]\n\n# Run t-tests\nt_1_vs_2 = ttest_ind(gave_1to1, gave_2to1)\nt_2_vs_3 = ttest_ind(gave_2to1, gave_3to1)\n\nt_1_vs_2, t_2_vs_3\n\n(TtestResult(statistic=-0.96504713432247, pvalue=0.33453168549723933, df=22265.0),\n TtestResult(statistic=-0.05011583793874515, pvalue=0.9600305283739325, df=22261.0))\n\n\nWe compare the response rates between different match ratios using t-tests. The comparison between the 1:1 and 2:1 match groups yields a p-value of approximately 0.335, and the comparison between 2:1 and 3:1 yields a p-value near 0.96. These results are not statistically significant at the 95 percent confidence level.\nThis means that increasing the match ratio from 1:1 to 2:1 or from 2:1 to 3:1 does not make individuals more likely to donate. The findings support the comment on page 8 of Karlan and List (2007) that larger match ratios do not meaningfully improve donation rates. The presence of a match itself seems to matter, but further increasing its size does not have additional impact on behavior.\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n#| label: regression-match-ratios\n#| message: false\n#| warning: false\n\nimport statsmodels.api as sm\n\n# ratio1 is the baseline group, so we drop it\nX = treated[[\"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = treated[\"gave\"]\n\nmatch_model = sm.OLS(y, X).fit()\nmatch_model.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|t|\"]]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nP&gt;|t|\n\n\n\n\nconst\n0.020749\n0.001391\n3.981333e-50\n\n\nratio2\n0.001884\n0.001968\n3.382805e-01\n\n\nratio3\n0.001984\n0.001968\n3.133172e-01\n\n\n\n\n\n\n\nWe regress the binary outcome variable gave on two dummy variables: ratio2 and ratio3, representing 2:1 and 3:1 match ratios respectively. The baseline group is ratio1, which corresponds to a 1:1 match.\nThe estimated coefficients on ratio2 and ratio3 are approximately 0.0019 and 0.0020. These values indicate that donors offered a 2:1 or 3:1 match were about 0.19 or 0.20 percentage points more likely to donate than those offered a 1:1 match. However, both estimates have p-values well above conventional significance thresholds: 0.338 for ratio2 and 0.313 for ratio3.\nThis lack of statistical significance means we cannot rule out the possibility that the observed differences are due to chance. The results suggest that increasing the match ratio from 1:1 to 2:1 or 3:1 does not lead to a meaningful increase in the probability of donation.\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n#| label: match-ratio-differences-2\n#| message: false\n#| warning: false\n\n# Direct differences in donation rates from data\nrate_diff_2_vs_1 = gave_2to1.mean() - gave_1to1.mean()\nrate_diff_3_vs_2 = gave_3to1.mean() - gave_2to1.mean()\n\n# Differences from regression coefficients\ncoef_2_vs_1 = match_model.params[\"ratio2\"]  # relative to baseline (1:1)\ncoef_3_vs_2 = match_model.params[\"ratio3\"] - match_model.params[\"ratio2\"]\n\n(rate_diff_2_vs_1, rate_diff_3_vs_2), (coef_2_vs_1, coef_3_vs_2)\n\n((0.0018842510217149944, 0.00010002398025293902),\n (0.0018842510217141856, 0.00010002398025350584))\n\n\nThe difference in donation rates between the 2:1 and 1:1 match ratios is approximately 0.19 percentage points, and between the 3:1 and 2:1 match ratios is about 0.01 percentage points. These differences are very small and not statistically significant.\nBoth the direct calculation from the data and the regression-based estimates lead to the same conclusion: increasing the size of the match offer does not meaningfully increase the likelihood that a donor gives. This implies that larger matching ratios are not more effective than a simple 1:1 match.\nThese results reinforce the interpretation in Karlan and List (2007) that what matters is the presence of a match, not how generous it is. Fundraising efforts may be better off focusing on whether to include a match offer rather than trying to optimize its size.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n#| label: amount-regression-full\n#| message: false\n#| warning: false\n\nimport statsmodels.api as sm\n\n# Define inputs\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"amount\"]\n\n# Fit model\namount_model = sm.OLS(y, X).fit()\namount_model.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|t|\"]]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nP&gt;|t|\n\n\n\n\nconst\n0.813268\n0.067418\n1.843438e-33\n\n\ntreatment\n0.153605\n0.082561\n6.282029e-02\n\n\n\n\n\n\n\nWe ran a regression of donation amount on treatment status using the full sample, which includes both donors and non-donors.\nThe estimated treatment effect is approximately 0.15 dollars, meaning that individuals in the treatment group gave slightly more on average than those in the control group. However, the p-value is around 0.063, which is slightly above the conventional 0.05 threshold for statistical significance.\nThis means that we cannot confidently conclude that the matching donation offer increases the total amount donated. The result suggests a possible positive effect, but it is not statistically robust enough to rule out chance. It appears that the treatment might have a small effect on total giving, but further evidence would be needed to confirm it.\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n#| label: amount-regression-full\n#| message: false\n#| warning: false\n\nimport statsmodels.api as sm\n\n# Define inputs\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"amount\"]\n\n# Fit model\namount_model = sm.OLS(y, X).fit()\namount_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.461\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.0628\n\n\nTime:\n22:38:08\nLog-Likelihood:\n-1.7946e+05\n\n\nNo. Observations:\n50083\nAIC:\n3.589e+05\n\n\nDf Residuals:\n50081\nBIC:\n3.589e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n0.8133\n0.067\n12.063\n0.000\n0.681\n0.945\n\n\ntreatment\n0.1536\n0.083\n1.861\n0.063\n-0.008\n0.315\n\n\n\n\n\n\n\n\nOmnibus:\n96861.113\nDurbin-Watson:\n2.008\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n240735713.635\n\n\nSkew:\n15.297\nProb(JB):\n0.00\n\n\nKurtosis:\n341.269\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe ran a linear regression of the amount donated on treatment status, using the full sample that includes individuals who did not donate.\nThe treatment coefficient is approximately 0.15, suggesting that individuals in the treatment group gave about 15 cents more on average than those in the control group. However, this estimate is only marginally statistically significant, with a p-value of 0.063.\nThis result provides some weak evidence that matching offers may increase average donations, but it is not strong enough to be conclusive.\nBecause this analysis includes non-donors whose amount is zero, the estimated coefficient captures both the treatment’s effect on the likelihood of giving and on the amount given conditional on giving. As a result, the coefficient does not have a clear causal interpretation for donation size alone. It blends two effects: whether people donate and how much they donate if they do. To isolate the effect on donation size, we need to restrict the sample to donors only.\n\n\n\n\n\n\n\n\n\nWe create separate histograms of donation amounts for the control and treatment groups, including only individuals who made a donation. A red vertical line marks the mean donation amount in each group.\nThe shapes of the distributions are similar, with most donations concentrated at smaller amounts. Both groups have a long right tail due to a small number of larger donations. The average donation in the treatment group appears slightly lower than in the control group.\nThis visual impression is consistent with our earlier regression analysis, which showed no statistically significant treatment effect on the donation amount, conditional on giving. Overall, the match offer does not appear to influence how much people give once they decide to donate."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#simulation-experiment",
    "href": "projects/project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\nThe plot shows the cumulative average of the difference in outcomes between simulated treatment and control groups over 10,000 draws. Each individual simulation is based on a Bernoulli process with probabilities 0.022 for the treatment group and 0.018 for the control group.\nInitially, the cumulative average fluctuates widely, but as the number of simulations increases, the fluctuations diminish and the average converges toward the true difference in means, which is 0.004.\nThis demonstrates the Law of Large Numbers. As the sample size grows, the sample average becomes a more reliable estimate of the expected value. In this case, with enough simulations, the average observed treatment effect approximates the true treatment effect.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\nThe histograms show the distribution of the difference in means between treatment and control groups across 1000 simulated experiments, for sample sizes of 50, 200, 500, and 1000.\nAs the sample size increases, the distribution becomes more concentrated around the true average difference. At smaller sample sizes such as 50, the distribution is wide, and zero lies well within the center. This reflects a high degree of uncertainty in small samples.\nAt larger sample sizes, especially n = 1000, the distribution becomes narrower and more symmetric, approximating a normal distribution. In these cases, zero is closer to the edge or tail of the distribution. This means that with larger samples, we are less likely to observe a difference near zero if the true difference is positive, as it is in this simulation.\nOverall, this confirms the Central Limit Theorem and illustrates how increased sample size improves our ability to detect small but real differences between groups."
  },
  {
    "objectID": "projects/project3/hw2_questions.html",
    "href": "projects/project3/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nWe observe that the distribution of Blueprinty customers across regions is not uniform. For example, the Northeast has a noticeably higher proportion of customers compared to other regions, while the Midwest and South appear to have relatively fewer. This suggests that region may be correlated with customer status and should be accounted for in later analysis to avoid confounding.\n\n\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nThe age distribution of customers and non-customers shows a meaningful difference. On average, customers are older: the mean age of customers is about 26.9 years, compared to 26.1 years for non-customers. This is also visible in the histogram, where the customer age distribution is skewed slightly to the right. Non-customers are more concentrated in the 20–30 age range, while customers have a wider spread including more older firms. This suggests that age is correlated with customer status, and it reinforces the importance of controlling for age in the regression analysis.\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents \\(Y\\) follows a Poisson distribution with parameter \\(\\lambda\\):\n\\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe log-likelihood function is:\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_i \\left[ -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right]\n\\]\nTo evaluate this log-likelihood numerically, we define a Python function:\n\nimport numpy as np\nfrom scipy.special import gammaln\nimport pandas as pd\n\n# Load blueprinty data\ndf = pd.read_csv(\"../project3/blueprinty.csv\")\n\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\n# Extract Y data\nY = df[\"patents\"].values\n\nWe can then plot this function over a range of lambda values to understand where the likelihood is maximized.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load blueprinty data\ndf = pd.read_csv(\"../project3/blueprinty.csv\")\n\n# Try a range of lambda values\nlambda_values = np.linspace(0.1, 10, 200)\nloglik_values = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_values]\n\n# Plot\nplt.figure(figsize=(8, 4))\nplt.plot(lambda_values, loglik_values)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood for Different Lambda Values\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTaking the derivative of the log-likelihood function:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_i Y_i\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_i Y_i = 0 \\Rightarrow \\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_i Y_i = \\bar{Y}\n\\]\nSo the MLE of \\(\\lambda\\) is simply the sample mean.\n\n# Sample mean of patents = MLE for lambda\nlambda_mle = df[\"patents\"].mean()\nlambda_mle\n\n3.6846666666666668\n\n\nTo confirm our analytical result, we now estimate the MLE of \\(\\lambda\\) numerically using scipy.optimize.\n\nfrom scipy.optimize import minimize\n\n# Negative log-likelihood for minimization\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda[0], Y)\n\n# Use optimizer to find lambda MLE\nresult = minimize(neg_loglik, x0=[2], bounds=[(1e-5, None)])\nresult\n\n  message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n  success: True\n   status: 0\n      fun: 3367.683772235095\n        x: [ 3.685e+00]\n      nit: 6\n      jac: [-4.547e-05]\n     nfev: 14\n     njev: 7\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\nWe verify the MLE numerically by using optimization to minimize the negative log-likelihood. The optimizer returns a value very close to the sample mean of Y, confirming our analytical derivation.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe begin by generalizing our Poisson log-likelihood function to incorporate covariates. Instead of estimating a single \\(\\lambda\\), we now assume that each firm \\(i\\) has its own rate \\(\\lambda_i = \\exp(X_i'\\beta)\\), where \\(X\\) is the matrix of covariates and \\(\\beta\\) is the parameter vector.\nWe modify our likelihood function to take both \\(X\\) and \\(\\beta\\) as inputs, and use the exponential link to ensure all predicted rates remain positive.\n\nimport numpy as np\n\n# Poisson regression log-likelihood\ndef poisson_regression_loglik(beta, Y, X):\n    Xb = X @ beta\n    lambdas = np.exp(Xb)\n    loglik = np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n    return -loglik  # negative log-likelihood for minimization\n\nTo estimate the Poisson regression model, we numerically maximize the log-likelihood with respect to the parameter vector \\(\\beta\\). We use Python’s optimization routine to find the MLE and obtain the Hessian matrix at the optimum.\nWe then use the inverse of the negative Hessian to approximate the variance-covariance matrix of the estimates. The square roots of the diagonal entries give us standard errors for each coefficient.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\ndf = pd.read_csv(\"../project3/blueprinty.csv\")\n\n# Create age_squared\ndf[\"age_squared\"] = df[\"age\"] ** 2\n\n# Create region dummy variables (drop Midwest as baseline automatically)\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Merge region dummies into df\ndf = pd.concat([df, region_dummies], axis=1)\n\n# Define outcome variable\nY = df[\"patents\"].values\n\n# Define covariates (excluding one region dummy as baseline)\nX = df[[\"age\", \"age_squared\", \"region_Northeast\", \"region_Northwest\", \"region_South\", \"region_Southwest\", \"iscustomer\"]].copy()\n\n# Add intercept\nX.insert(0, \"intercept\", 1)\nX = X.values\n\n# Define Poisson regression log-likelihood\ndef poisson_regression_loglik(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    X = np.asarray(X, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n\n    # Ensure dot product result is a 1D array\n    Xb = np.dot(X, beta).flatten()\n\n    # This will now safely apply exp to each element\n    lambdas = np.exp(Xb)\n\n    loglik = np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n    return -loglik\n\n# Initial guess for beta\ninit_beta = np.zeros(X.shape[1])\n\n# Run optimization\nres = minimize(poisson_regression_loglik, init_beta, args=(Y, X), method=\"BFGS\")\n\n# Extract estimated coefficients\nbeta_hat = res.x\n\n# Extract inverse Hessian\nhessian_inv = res.hess_inv\n\n# Compute standard errors\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nAs a validation step, we fit the same model using Python’s built-in generalized linear model (GLM) function with a Poisson family. We compare the estimated coefficients and standard errors from our custom likelihood with the results from sm.GLM to confirm consistency.\n\nimport statsmodels.api as sm\n\n# Select relevant covariates and convert to float\nX_glm = df[[\"age\", \"age_squared\", \"region_Northeast\", \"region_Northwest\", \"region_South\", \"region_Southwest\", \"iscustomer\"]].astype(float)\n\n# Add intercept column\nX_glm = sm.add_constant(X_glm)\n\n# Convert Y to float\nY_glm = df[\"patents\"].astype(float)\n\n# Fit Poisson regression model\nglm_model = sm.GLM(Y_glm, X_glm, family=sm.families.Poisson()).fit()\n\n# Output formatted regression table\nglm_model.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|z|\"]]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nP&gt;|z|\n\n\n\n\nconst\n-0.508920\n0.183179\n5.464935e-03\n\n\nage\n0.148619\n0.013869\n8.539597e-27\n\n\nage_squared\n-0.002970\n0.000258\n1.131496e-30\n\n\nregion_Northeast\n0.029170\n0.043625\n5.037205e-01\n\n\nregion_Northwest\n-0.017575\n0.053781\n7.438327e-01\n\n\nregion_South\n0.056561\n0.052662\n2.828066e-01\n\n\nregion_Southwest\n0.050576\n0.047198\n2.839141e-01\n\n\niscustomer\n0.207591\n0.030895\n1.827509e-11\n\n\n\n\n\n\n\nThe estimated coefficients indicate how each variable affects the expected number of patents, on the log scale.\nThe coefficient on age is positive and statistically significant, suggesting that older firms tend to receive more patents. The negative coefficient on age_squared indicates a concave relationship: the marginal effect of age decreases with age.\nRegion variables are not statistically significant, meaning that geographic location (relative to the baseline) does not have a strong effect on patent counts.\nMost importantly, the coefficient on iscustomer is positive and highly significant (p &lt; 0.001). This implies that, controlling for other firm characteristics, Blueprinty customers tend to receive more patents than non-customers.\nTo better understand the effect of using Blueprinty’s software, we simulate two hypothetical scenarios:\n\n\\(X_0\\): all firms are treated as non-customers (iscustomer = 0)\n\n\\(X_1\\): all firms are treated as customers (iscustomer = 1)\n\nWe use our fitted model to generate predicted patent counts under both scenarios, and calculate the average difference. This provides an interpretable estimate of the average treatment effect of using the software.\n\n# Make sure beta_hat is numpy array\nbeta_hat = glm_model.params.to_numpy()\n\n# Reconstruct X\nX_full = df[[\"age\", \"age_squared\", \"region_Northeast\", \"region_Northwest\", \"region_South\", \"region_Southwest\", \"iscustomer\"]].copy()\nX_full.insert(0, \"intercept\", 1)\n\n# Build X_0 (non-customers)\nX_0 = X_full.copy()\nX_0[\"iscustomer\"] = 0\n\n# Build X_1 (customers)\nX_1 = X_full.copy()\nX_1[\"iscustomer\"] = 1\n\n# Convert to numpy matrices\nX_0 = X_0.to_numpy(dtype=float)\nX_1 = X_1.to_numpy(dtype=float)\n\n# Predict expected patent counts\nlambda_0 = np.exp(X_0 @ beta_hat)\nlambda_1 = np.exp(X_1 @ beta_hat)\n\n# Average treatment effect\ntreatment_effect = np.mean(lambda_1 - lambda_0)\ntreatment_effect\n\n0.7927680710452553\n\n\nTo estimate the average treatment effect of Blueprinty’s software, we simulated two counterfactual datasets: one where all firms are treated as non-customers and one where all are treated as customers. Using our fitted Poisson regression model, we predicted the number of patents in each scenario.\nOn average, being a customer is associated with an increase of approximately 0.79 patents over 5 years. This suggests that Blueprinty’s software may positively impact patent success, even after accounting for other firm characteristics."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nWe observe that the distribution of Blueprinty customers across regions is not uniform. For example, the Northeast has a noticeably higher proportion of customers compared to other regions, while the Midwest and South appear to have relatively fewer. This suggests that region may be correlated with customer status and should be accounted for in later analysis to avoid confounding.\n\n\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nThe age distribution of customers and non-customers shows a meaningful difference. On average, customers are older: the mean age of customers is about 26.9 years, compared to 26.1 years for non-customers. This is also visible in the histogram, where the customer age distribution is skewed slightly to the right. Non-customers are more concentrated in the 20–30 age range, while customers have a wider spread including more older firms. This suggests that age is correlated with customer status, and it reinforces the importance of controlling for age in the regression analysis.\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents \\(Y\\) follows a Poisson distribution with parameter \\(\\lambda\\):\n\\[\nf(Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe log-likelihood function is:\n\\[\n\\log L(\\lambda \\mid Y) = \\sum_i \\left[ -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right]\n\\]\nTo evaluate this log-likelihood numerically, we define a Python function:\n\nimport numpy as np\nfrom scipy.special import gammaln\nimport pandas as pd\n\n# Load blueprinty data\ndf = pd.read_csv(\"../project3/blueprinty.csv\")\n\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\n# Extract Y data\nY = df[\"patents\"].values\n\nWe can then plot this function over a range of lambda values to understand where the likelihood is maximized.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load blueprinty data\ndf = pd.read_csv(\"../project3/blueprinty.csv\")\n\n# Try a range of lambda values\nlambda_values = np.linspace(0.1, 10, 200)\nloglik_values = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_values]\n\n# Plot\nplt.figure(figsize=(8, 4))\nplt.plot(lambda_values, loglik_values)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood for Different Lambda Values\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTaking the derivative of the log-likelihood function:\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_i Y_i\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_i Y_i = 0 \\Rightarrow \\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_i Y_i = \\bar{Y}\n\\]\nSo the MLE of \\(\\lambda\\) is simply the sample mean.\n\n# Sample mean of patents = MLE for lambda\nlambda_mle = df[\"patents\"].mean()\nlambda_mle\n\n3.6846666666666668\n\n\nTo confirm our analytical result, we now estimate the MLE of \\(\\lambda\\) numerically using scipy.optimize.\n\nfrom scipy.optimize import minimize\n\n# Negative log-likelihood for minimization\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda[0], Y)\n\n# Use optimizer to find lambda MLE\nresult = minimize(neg_loglik, x0=[2], bounds=[(1e-5, None)])\nresult\n\n  message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n  success: True\n   status: 0\n      fun: 3367.683772235095\n        x: [ 3.685e+00]\n      nit: 6\n      jac: [-4.547e-05]\n     nfev: 14\n     njev: 7\n hess_inv: &lt;1x1 LbfgsInvHessProduct with dtype=float64&gt;\n\n\nWe verify the MLE numerically by using optimization to minimize the negative log-likelihood. The optimizer returns a value very close to the sample mean of Y, confirming our analytical derivation.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe begin by generalizing our Poisson log-likelihood function to incorporate covariates. Instead of estimating a single \\(\\lambda\\), we now assume that each firm \\(i\\) has its own rate \\(\\lambda_i = \\exp(X_i'\\beta)\\), where \\(X\\) is the matrix of covariates and \\(\\beta\\) is the parameter vector.\nWe modify our likelihood function to take both \\(X\\) and \\(\\beta\\) as inputs, and use the exponential link to ensure all predicted rates remain positive.\n\nimport numpy as np\n\n# Poisson regression log-likelihood\ndef poisson_regression_loglik(beta, Y, X):\n    Xb = X @ beta\n    lambdas = np.exp(Xb)\n    loglik = np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n    return -loglik  # negative log-likelihood for minimization\n\nTo estimate the Poisson regression model, we numerically maximize the log-likelihood with respect to the parameter vector \\(\\beta\\). We use Python’s optimization routine to find the MLE and obtain the Hessian matrix at the optimum.\nWe then use the inverse of the negative Hessian to approximate the variance-covariance matrix of the estimates. The square roots of the diagonal entries give us standard errors for each coefficient.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\ndf = pd.read_csv(\"../project3/blueprinty.csv\")\n\n# Create age_squared\ndf[\"age_squared\"] = df[\"age\"] ** 2\n\n# Create region dummy variables (drop Midwest as baseline automatically)\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Merge region dummies into df\ndf = pd.concat([df, region_dummies], axis=1)\n\n# Define outcome variable\nY = df[\"patents\"].values\n\n# Define covariates (excluding one region dummy as baseline)\nX = df[[\"age\", \"age_squared\", \"region_Northeast\", \"region_Northwest\", \"region_South\", \"region_Southwest\", \"iscustomer\"]].copy()\n\n# Add intercept\nX.insert(0, \"intercept\", 1)\nX = X.values\n\n# Define Poisson regression log-likelihood\ndef poisson_regression_loglik(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    X = np.asarray(X, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n\n    # Ensure dot product result is a 1D array\n    Xb = np.dot(X, beta).flatten()\n\n    # This will now safely apply exp to each element\n    lambdas = np.exp(Xb)\n\n    loglik = np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n    return -loglik\n\n# Initial guess for beta\ninit_beta = np.zeros(X.shape[1])\n\n# Run optimization\nres = minimize(poisson_regression_loglik, init_beta, args=(Y, X), method=\"BFGS\")\n\n# Extract estimated coefficients\nbeta_hat = res.x\n\n# Extract inverse Hessian\nhessian_inv = res.hess_inv\n\n# Compute standard errors\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nAs a validation step, we fit the same model using Python’s built-in generalized linear model (GLM) function with a Poisson family. We compare the estimated coefficients and standard errors from our custom likelihood with the results from sm.GLM to confirm consistency.\n\nimport statsmodels.api as sm\n\n# Select relevant covariates and convert to float\nX_glm = df[[\"age\", \"age_squared\", \"region_Northeast\", \"region_Northwest\", \"region_South\", \"region_Southwest\", \"iscustomer\"]].astype(float)\n\n# Add intercept column\nX_glm = sm.add_constant(X_glm)\n\n# Convert Y to float\nY_glm = df[\"patents\"].astype(float)\n\n# Fit Poisson regression model\nglm_model = sm.GLM(Y_glm, X_glm, family=sm.families.Poisson()).fit()\n\n# Output formatted regression table\nglm_model.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"P&gt;|z|\"]]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nP&gt;|z|\n\n\n\n\nconst\n-0.508920\n0.183179\n5.464935e-03\n\n\nage\n0.148619\n0.013869\n8.539597e-27\n\n\nage_squared\n-0.002970\n0.000258\n1.131496e-30\n\n\nregion_Northeast\n0.029170\n0.043625\n5.037205e-01\n\n\nregion_Northwest\n-0.017575\n0.053781\n7.438327e-01\n\n\nregion_South\n0.056561\n0.052662\n2.828066e-01\n\n\nregion_Southwest\n0.050576\n0.047198\n2.839141e-01\n\n\niscustomer\n0.207591\n0.030895\n1.827509e-11\n\n\n\n\n\n\n\nThe estimated coefficients indicate how each variable affects the expected number of patents, on the log scale.\nThe coefficient on age is positive and statistically significant, suggesting that older firms tend to receive more patents. The negative coefficient on age_squared indicates a concave relationship: the marginal effect of age decreases with age.\nRegion variables are not statistically significant, meaning that geographic location (relative to the baseline) does not have a strong effect on patent counts.\nMost importantly, the coefficient on iscustomer is positive and highly significant (p &lt; 0.001). This implies that, controlling for other firm characteristics, Blueprinty customers tend to receive more patents than non-customers.\nTo better understand the effect of using Blueprinty’s software, we simulate two hypothetical scenarios:\n\n\\(X_0\\): all firms are treated as non-customers (iscustomer = 0)\n\n\\(X_1\\): all firms are treated as customers (iscustomer = 1)\n\nWe use our fitted model to generate predicted patent counts under both scenarios, and calculate the average difference. This provides an interpretable estimate of the average treatment effect of using the software.\n\n# Make sure beta_hat is numpy array\nbeta_hat = glm_model.params.to_numpy()\n\n# Reconstruct X\nX_full = df[[\"age\", \"age_squared\", \"region_Northeast\", \"region_Northwest\", \"region_South\", \"region_Southwest\", \"iscustomer\"]].copy()\nX_full.insert(0, \"intercept\", 1)\n\n# Build X_0 (non-customers)\nX_0 = X_full.copy()\nX_0[\"iscustomer\"] = 0\n\n# Build X_1 (customers)\nX_1 = X_full.copy()\nX_1[\"iscustomer\"] = 1\n\n# Convert to numpy matrices\nX_0 = X_0.to_numpy(dtype=float)\nX_1 = X_1.to_numpy(dtype=float)\n\n# Predict expected patent counts\nlambda_0 = np.exp(X_0 @ beta_hat)\nlambda_1 = np.exp(X_1 @ beta_hat)\n\n# Average treatment effect\ntreatment_effect = np.mean(lambda_1 - lambda_0)\ntreatment_effect\n\n0.7927680710452553\n\n\nTo estimate the average treatment effect of Blueprinty’s software, we simulated two counterfactual datasets: one where all firms are treated as non-customers and one where all are treated as customers. Using our fitted Poisson regression model, we predicted the number of patents in each scenario.\nOn average, being a customer is associated with an increase of approximately 0.79 patents over 5 years. This suggests that Blueprinty’s software may positively impact patent success, even after accounting for other firm characteristics."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#airbnb-case-study",
    "href": "projects/project3/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not"
  },
  {
    "objectID": "projects/project3/hw2_questions.html#exploratory-data-analysis",
    "href": "projects/project3/hw2_questions.html#exploratory-data-analysis",
    "title": "Poisson Regression Examples",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe outcome variable number_of_reviews is highly right-skewed, with most listings receiving very few reviews. The median is only 8 reviews, while the maximum exceeds 400. This justifies the use of a count model such as Poisson regression.\nThe distribution of price is also extremely skewed. While the median nightly rate is around $103, the maximum reaches $10,000. This indicates the presence of substantial outliers, and we may consider applying a log transformation before modeling.\nReview scores (cleanliness, location, value) are generally high and compressed toward the upper bound of 10. The days variable, representing how long a listing has been active, varies widely across listings, with a mean of about 3 years.\nOverall, this initial exploration supports our modeling strategy using number_of_reviews as the dependent variable and listing characteristics as predictors.\n\n\n\n\n\n\n\n\n\nnumber_of_reviews\ndays\nbathrooms\nbedrooms\nprice\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\nroom_type_Private room\nroom_type_Shared room\n\n\n\n\n0\n150\n3130\n1.0\n1.0\n59\n9.0\n9.0\n9.0\n0\nTrue\nFalse\n\n\n1\n20\n3127\n1.0\n0.0\n230\n9.0\n10.0\n9.0\n0\nFalse\nFalse\n\n\n3\n116\n3038\n1.0\n1.0\n89\n9.0\n9.0\n9.0\n0\nFalse\nFalse\n\n\n5\n60\n2981\n1.0\n1.0\n212\n9.0\n9.0\n9.0\n0\nFalse\nFalse\n\n\n6\n60\n2981\n1.0\n2.0\n250\n10.0\n9.0\n10.0\n0\nFalse\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumber_of_reviews\nprice\ndays\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n\n\nmean\n21.170889\n140.206863\n1139.711174\n9.201724\n9.415351\n9.333952\n\n\nstd\n32.007541\n188.392314\n1252.303675\n1.114261\n0.843185\n0.900472\n\n\nmin\n1.000000\n10.000000\n7.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n3.000000\n70.000000\n584.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n8.000000\n103.000000\n1041.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n26.000000\n169.000000\n1592.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n421.000000\n10000.000000\n42828.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\nThe exploratory visualizations provide further insight into how listing features relate to the number of reviews.\nListings with moderate prices tend to receive more reviews than those at the extreme low or high ends. This suggests that both underpricing and overpricing may reduce demand, creating a curved relationship between price and reviews.\nAs expected, listings that have been active on the platform longer generally accumulate more reviews. However, there are a few outliers—listings with thousands of days online but very few reviews—which may indicate inactive or underperforming hosts.\nWhen comparing listings with and without instant booking, those that allow instant booking clearly receive more reviews. This suggests that easier booking access is associated with increased guest activity.\nFinally, the correlation heatmap shows that review score variables (cleanliness, location, value) are moderately correlated with one another, but all other variables exhibit relatively weak correlations. This implies that multicollinearity is not a major concern for modeling."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#data-preparation",
    "href": "projects/project3/hw2_questions.html#data-preparation",
    "title": "Poisson Regression Examples",
    "section": "Data Preparation",
    "text": "Data Preparation\nNext, we clean the dataset by removing observations with missing values in key variables. This ensures that our model is trained on complete cases without introducing bias or instability due to nulls.\n\n# Drop rows with missing values\ndf_airbnb_clean = df_airbnb.dropna()\n\n# Log-transform price\ndf_airbnb_clean[\"log_price\"] = np.log1p(df_airbnb_clean[\"price\"])\n\n# Review again after cleaning\ndf_airbnb_clean.describe()\n\n\n\n\n\n\n\n\nnumber_of_reviews\ndays\nbathrooms\nbedrooms\nprice\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\nlog_price\n\n\n\n\ncount\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n30160.000000\n\n\nmean\n21.170889\n1139.711174\n1.122132\n1.151459\n140.206863\n9.201724\n9.415351\n9.333952\n0.196187\n4.712802\n\n\nstd\n32.007541\n1252.303675\n0.384916\n0.699010\n188.392314\n1.114261\n0.843185\n0.900472\n0.397118\n0.637097\n\n\nmin\n1.000000\n7.000000\n0.000000\n0.000000\n10.000000\n2.000000\n2.000000\n2.000000\n0.000000\n2.397895\n\n\n25%\n3.000000\n584.000000\n1.000000\n1.000000\n70.000000\n9.000000\n9.000000\n9.000000\n0.000000\n4.262680\n\n\n50%\n8.000000\n1041.000000\n1.000000\n1.000000\n103.000000\n10.000000\n10.000000\n10.000000\n0.000000\n4.644391\n\n\n75%\n26.000000\n1592.000000\n1.000000\n1.000000\n169.000000\n10.000000\n10.000000\n10.000000\n0.000000\n5.135798\n\n\nmax\n421.000000\n42828.000000\n6.000000\n10.000000\n10000.000000\n10.000000\n10.000000\n10.000000\n1.000000\n9.210440"
  },
  {
    "objectID": "projects/project3/hw2_questions.html#coefficient-interpretation",
    "href": "projects/project3/hw2_questions.html#coefficient-interpretation",
    "title": "Poisson Regression Examples",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nWe interpret the estimated coefficients from the Poisson regression model in terms of their impact on the expected number of reviews.\n\nlog_price: A one-unit increase in log(price) is associated with a 12.2% increase in expected review count, holding other variables constant. This suggests that more expensive listings (on a log scale) tend to receive more reviews.\ndays: The longer a listing has been on the platform, the more reviews it accumulates. The effect is small per day but statistically significant.\nreview_scores_cleanliness: Higher cleanliness ratings are positively associated with review counts, as expected.\nreview_scores_location and review_scores_value: Interestingly, both coefficients are negative, which may reflect multicollinearity or possible nonlinear patterns in user behavior.\ninstant_bookable: Listings that allow instant booking are associated with 35% higher expected review counts, indicating that booking convenience improves performance.\nroom_type_Private room: Compared to entire homes/apartments (the baseline), private rooms receive slightly more reviews.\nroom_type_Shared room: Shared rooms receive fewer reviews on average compared to the baseline, with a significant negative effect.\n\nOverall, the model reveals that price, availability, cleanliness, and room type meaningfully explain variation in the number of reviews a listing receives."
  }
]